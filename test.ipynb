{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import xlrd\n",
    "import re\n",
    "import csv\n",
    "import fasttext\n",
    "import traceback\n",
    "import random\n",
    "import utils\n",
    "import json\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from utils import fixText\n",
    "\n",
    "def getConfig():\n",
    "    config = {}\n",
    "    with open('./config.json', 'r') as f:\n",
    "        s = f.read()\n",
    "        config = json.loads(s)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.tsv', 'r') as f:\n",
    "    content = f.readlines()\n",
    "    random.shuffle(content)\n",
    "    train = content[0:len(content) // 5 * 4]\n",
    "    test = content[len(content) // 5 * 4:]\n",
    "    \n",
    "    with open('data/old_train_set.txt', 'w') as out:\n",
    "        for c in train:\n",
    "            res = re.search(r'__label__(.*)[^\\r\\n]', c)\n",
    "            label = c[res.span()[0]:res.span()[1]]\n",
    "            c = re.sub(r'__label__(.*)', '', c)\n",
    "            c = ''.join(re.split('\\t| |\\r|\\n', c))\n",
    "            c = fixText(c)\n",
    "            out.write(c + ' ' + label + '\\n')\n",
    "    with open('data/old_test_set.txt', 'w') as out:\n",
    "        for c in test:\n",
    "            res = re.search(r'__label__(.*)[^\\r\\n]', c)\n",
    "            label = c[res.span()[0]:res.span()[1]]\n",
    "            c = re.sub(r'__label__(.*)', '', c)\n",
    "            c = ''.join(re.split('\\t| |\\r|\\n', c))\n",
    "            c = fixText(c)\n",
    "            # c = ''.join(c.split(' '))\n",
    "            out.write(c + ' ' + label + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = getConfig()\n",
    "model = fasttext.train_supervised(\n",
    "    input = './data/old_train_set.txt',\n",
    "    lr = config['lr'],\n",
    "    dim = config['hidden_dim'],\n",
    "    epoch = config['epoch']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "true_tag = 0\n",
    "with open('./data/old_test_set.txt', 'r') as f:\n",
    "    content = f.readlines()\n",
    "    total = len(content)\n",
    "    for c in content:\n",
    "        res = re.search(r'__label__(.*)[^\\r\\n]', c)\n",
    "        label = c[res.span()[0]:res.span()[1]]\n",
    "        c = re.sub(r'__label__(.*)', '', c)\n",
    "        predict = model.predict(c[:-1])[0][0]\n",
    "        \n",
    "        print(model.predict(c[:-1]))\n",
    "        # print(predict, label)\n",
    "        if(predict == label):\n",
    "            true_tag += 1\n",
    "        break\n",
    "#print('tag level 1 accurate: {}% ({}/{})'.format(true_tag * 100 / total, true_tag, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def resetLabelLv2(label):\n",
    "    if(re.search(r'.+街(道)*', label)):\n",
    "        label = re.search(r'.+街(道)*', label).group()\n",
    "        if \"街道\" not in label:\n",
    "            label = label.replace(\"街\", \"街道\")\n",
    "        if(re.search(r'.+[区|市]', label)):\n",
    "            label = re.sub(re.search(r'.+[区|市]', label).group(), '', label)\n",
    "    return label\n",
    "\n",
    "def resetLabelLv3(label):\n",
    "    if(re.search(r'[\\u4e00-\\u9fa50-9]+街(道)*', label)):\n",
    "        label = re.search(r'街(道)*.*', label).group()\n",
    "        label = re.sub(r'[(|（][\\u4e00-\\u9fa50-9]*[)|）]', '', label)\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr = '广州市白云区区鹤洞街办事处假装有这个分队(测试一下括号内）测试一下括号外（测试一下多个括号）'\n",
    "addr2 = resetLabelLv2(addr)\n",
    "addr3 = resetLabelLv3(addr)\n",
    "print(addr2)\n",
    "print(addr3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import resetLabelLv2, resetLabelLv3\n",
    "with open('./data/train_set.json', 'r') as f:\n",
    "    s = f.read()\n",
    "    data_set = json.loads(s)\n",
    "label1 = {}\n",
    "label2 = {}\n",
    "label3 = {}\n",
    "\n",
    "for id in list(data_set.keys()):\n",
    "    if data_set[id]['tag_level_1'] in label1:\n",
    "        label1[data_set[id]['tag_level_1']] += 1\n",
    "    else:\n",
    "        label1[data_set[id]['tag_level_1']] = 1\n",
    "\n",
    "    tag = resetLabelLv2(data_set[id]['tag_level_2'])\n",
    "    if tag in label2:\n",
    "        label2[tag] += 1\n",
    "    else:\n",
    "        label2[tag] = 1\n",
    "\n",
    "    tag = resetLabelLv3(data_set[id]['tag_level_3'])\n",
    "    if tag in label3:\n",
    "        label3[tag] += 1\n",
    "    else:\n",
    "        label3[tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(len(label1))\n",
    "print(label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "146222"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.FastText.load_model('./model/model_for_label_2.model')\n",
    "len(model.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_text(text):\n",
    "    splited_text = re.split(r'\\n+|\\r+|\\r\\n|<br>', text)\n",
    "    while '' in splited_text:\n",
    "        splited_text.remove('')\n",
    "    return splited_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'def', '', '', '', 'ghi']\n",
      "['abc', 'def', 'ghi']\n"
     ]
    }
   ],
   "source": [
    "s = \"abc\\n\\ndef<br>\\n\\r\\nghi\"\n",
    "print(re.split(r'\\n+|\\r+|\\r\\n|<br>', s))\n",
    "print(split_text(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 旧数据处理，划分训练和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from utils import fixText\n",
    "data_path = '../dispatch/train_data/train.tsv'\n",
    "all_data = []\n",
    "with open(data_path, 'r') as f:\n",
    "    all_data = f.readlines()\n",
    "    random.shuffle(all_data)\n",
    "train_data = all_data[:len(all_data) // 5 * 4]\n",
    "with open('../dispatch/train_data/new_train.tsv', 'w') as f:\n",
    "    for item in train_data:\n",
    "        f.write(item)\n",
    "with open('./data/old_train_set.txt', 'w') as f:\n",
    "    for item in train_data:\n",
    "        res = re.search(r'__label__(.*)[^\\r\\n]', item)\n",
    "        label = item[res.span()[0]:res.span()[1]]\n",
    "        text = item[:res.span()[0]]\n",
    "        text = re.sub(r'[ |\\t]', '', text)\n",
    "        text = fixText(text)\n",
    "        f.write(label + ' ' + text + '\\n')\n",
    "\n",
    "test_data = all_data[len(all_data) // 5 * 4:]\n",
    "with open('../dispatch/train_data/new_test.tsv', 'w') as f:\n",
    "    for item in test_data:\n",
    "        f.write(item)\n",
    "with open('./data/old_test_set.txt', 'w') as f:\n",
    "    for item in test_data:\n",
    "        res = re.search(r'__label__(.*)[^\\r\\n]', item)\n",
    "        label = item[res.span()[0]:res.span()[1]]\n",
    "        text = item[:res.span()[0]]\n",
    "        text = re.sub(r'[ |\\t]', '', text)\n",
    "        text = fixText(text)\n",
    "        f.write(label + ' ' + text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上家数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "old_model = fasttext.load_model('../dispatch/models/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.24553366926249 % (14538/17464)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "true_cnt = 0\n",
    "total_num = 0\n",
    "with open('../dispatch/train_data/new_test.tsv', 'r') as f:\n",
    "    test_data = f.readlines()\n",
    "for item in test_data:\n",
    "    res = re.search(r'__label__(.*)[^\\r\\n]', item)\n",
    "    label = item[res.span()[0]:res.span()[1]]\n",
    "    text = re.sub(r'__label__(.*)[\\n]', '', item)\n",
    "    pre = old_model.predict(text)[0][0]\n",
    "    if label == pre:\n",
    "        true_cnt += 1\n",
    "    total_num += 1\n",
    "\n",
    "print('{} % ({}/{})'.format(true_cnt / total_num * 100, true_cnt, total_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们的模型应用到上家数据测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  96746\n",
      "Number of labels: 43\n",
      "Progress: 100.0% words/sec/thread:   22507 lr:  0.000000 avg.loss:  0.349475 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "from model import MyFastText\n",
    "\n",
    "model = MyFastText()\n",
    "model.train('./data/old_train_set.txt', lr = 0.1, dim = 256, epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.77233165368759 % (14630/17464)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "true_cnt = 0\n",
    "total_num = 0\n",
    "test_data = []\n",
    "with open('./data/old_test_set.txt', 'r') as f:\n",
    "    test_data = f.readlines()\n",
    "for item in test_data:\n",
    "    item = item.split(' ')\n",
    "    label = item[0]\n",
    "    text = ' '.join(item[1:])\n",
    "    pre = model.predict(text)[0][0]\n",
    "    if label == pre:\n",
    "        true_cnt += 1\n",
    "    total_num += 1\n",
    "\n",
    "print('{} % ({}/{})'.format(true_cnt / total_num * 100, true_cnt, total_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上家应用到新的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import resetLabelLv2, resetLabelLv3\n",
    "def readDataSet(config):\n",
    "    set1 = []\n",
    "    with open(config['data_path'] + config['dataset_label_1_name'], 'r') as f:\n",
    "        s = f.read()\n",
    "        data_set = json.loads(s)\n",
    "        for id in list(data_set.keys()):\n",
    "            text = data_set[id]['text']\n",
    "            # splited_text = split_text(text)\n",
    "            # for text in splited_text:\n",
    "            text = ''.join(fixText(text).split(' '))\n",
    "            set1.append(' '.join(list(text)) + ' ' + '__label__' + data_set[id]['label_level_1'])\n",
    "    \n",
    "    set2 = []\n",
    "    with open(config['data_path'] + config['dataset_label_2_name'], 'r') as f:\n",
    "        s = f.read()\n",
    "        data_set = json.loads(s)\n",
    "        for id in list(data_set.keys()):\n",
    "            text = data_set[id]['text']\n",
    "            # splited_text = split_text(text)\n",
    "            # for text in splited_text:\n",
    "            text = ''.join(fixText(text).split(' '))\n",
    "            set2.append(' '.join(list(text)) + ' ' + '__label__' + data_set[id]['label_level_2'])\n",
    "    \n",
    "    set3 = []\n",
    "    with open(config['data_path'] + config['dataset_label_3_name'], 'r') as f:\n",
    "        s = f.read()\n",
    "        data_set = json.loads(s)\n",
    "        for id in list(data_set.keys()):\n",
    "            text = data_set[id]['text']\n",
    "            # splited_text = split_text(text)\n",
    "            # for text in splited_text:\n",
    "            text = ''.join(fixText(text).split(' '))\n",
    "            set3.append(' '.join(list(text)) + ' ' + '__label__' + data_set[id]['label_level_3'])\n",
    "        \n",
    "    with open('../dispatch/train_data/set1.txt', 'w') as f:\n",
    "        print('训练集1拆分后共{}条文本'.format(len(set1)))\n",
    "        for l in set1:\n",
    "            f.write(l + '\\n')\n",
    "    with open('../dispatch/train_data/set2.txt', 'w') as f:\n",
    "        print('训练集2拆分后共{}条文本'.format(len(set2)))\n",
    "        for l in set2:\n",
    "            f.write(l + '\\n')\n",
    "    with open('../dispatch/train_data/set3.txt', 'w') as f:\n",
    "        print('训练集3拆分后共{}条文本'.format(len(set2)))\n",
    "        for l in set3:\n",
    "            f.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.726 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集1拆分后共153320条文本\n",
      "训练集2拆分后共153320条文本\n",
      "训练集3拆分后共153320条文本\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config = getConfig()\n",
    "readDataSet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from model import MyFastText\n",
    "import fasttext\n",
    "model_label1 = fasttext.load_model('../dispatch/models/model_for_set1.bin')\n",
    "model_label2 = fasttext.load_model('../dispatch/models/model_for_set2.bin')\n",
    "model_label3 = fasttext.load_model('../dispatch/models/model_for_set3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "predict processes one line at a time (remove '\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m label_2 \u001b[39m=\u001b[39m content[\u001b[39mid\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel_level_2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m label_3 \u001b[39m=\u001b[39m content[\u001b[39mid\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel_level_3\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m predict_1 \u001b[39m=\u001b[39m model_label1\u001b[39m.\u001b[39;49mpredict(text[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m predict_2 \u001b[39m=\u001b[39m model_label2\u001b[39m.\u001b[39mpredict(text[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m predict_3 \u001b[39m=\u001b[39m model_label3\u001b[39m.\u001b[39mpredict(text[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/fasttext/lib/python3.9/site-packages/fasttext/FastText.py:225\u001b[0m, in \u001b[0;36m_FastText.predict\u001b[0;34m(self, text, k, threshold, on_unicode_error)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mreturn\u001b[39;00m all_labels, all_probs\n\u001b[1;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     text \u001b[39m=\u001b[39m check(text)\n\u001b[1;32m    226\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mpredict(text, k, threshold, on_unicode_error)\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m predictions:\n",
      "File \u001b[0;32m~/miniconda3/envs/fasttext/lib/python3.9/site-packages/fasttext/FastText.py:212\u001b[0m, in \u001b[0;36m_FastText.predict.<locals>.check\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(entry):\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m--> 212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpredict processes one line at a time (remove \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39mn\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     entry \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m entry\n",
      "\u001b[0;31mValueError\u001b[0m: predict processes one line at a time (remove '\\n')"
     ]
    }
   ],
   "source": [
    "from utils import fixText\n",
    "config = getConfig()\n",
    "with open(config['data_path'] + 'test_set.json', 'r') as f:\n",
    "    content = json.loads(f.read())\n",
    "\n",
    "total = 0\n",
    "true_lable_1 = 0\n",
    "true_lable_2 = 0\n",
    "true_lable_3 = 0\n",
    "\n",
    "not_liwan_correct = 0\n",
    "not_liwan_total = 0\n",
    "\n",
    "for id in content.keys():\n",
    "    text = content[id]['text']\n",
    "    \n",
    "    label_1 = content[id]['label_level_1']\n",
    "    label_2 = content[id]['label_level_2']\n",
    "    label_3 = content[id]['label_level_3']\n",
    "    \n",
    "    \n",
    "    predict_1 = model_label1.predict(fixtext(text))[0][0]\n",
    "    predict_2 = model_label2.predict(text[:-1])[0][0]\n",
    "    predict_3 = model_label3.predict(text[:-1])[0][0]\n",
    "\n",
    "    if(predict_1.replace('__label__', '') == label_1):\n",
    "        true_lable_1 += 1\n",
    "    if(predict_2.replace('__label__', '') == label_2):\n",
    "        true_lable_2 += 1\n",
    "    if(predict_3.replace('__label__', '') == label_3):\n",
    "        true_lable_3 += 1\n",
    "    total += 1\n",
    "\n",
    "    if('荔湾区政府' not in label_1):\n",
    "        not_liwan_total += 1\n",
    "        if(label_1 == predict_1):\n",
    "            not_liwan_correct += 1\n",
    "print('label level 1 accurate: {}% ({}/{})'.format(true_lable_1 * 100 / total, true_lable_1, total))\n",
    "print('tag level 2 accurate: {}% ({}/{})'.format(true_lable_2 * 100 / total, true_lable_2, total))\n",
    "print('tag level 3 accurate: {}% ({}/{})'.format(true_lable_3 * 100 / total, true_lable_3, total))\n",
    "\n",
    "print('{}% ({}/{})'.format(not_liwan_correct / not_liwan_total, not_liwan_correct, not_liwan_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f737a19b7090e33edbc76acd0a86580e3257e24fc38a119320c8b10678394dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
