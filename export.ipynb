{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paddle2ONNX] Start to parse PaddlePaddle model...\n",
      "[Paddle2ONNX] Model file path: ./model/export/float32.pdmodel\n",
      "[Paddle2ONNX] Paramters file path: ./model/export/float32.pdiparams\n",
      "[Paddle2ONNX] Start to parsing Paddle model...\n",
      "[Paddle2ONNX] Use opset_version = 13 for ONNX export.\n",
      "[Paddle2ONNX] PaddlePaddle model is exported as ONNX format now.\n"
     ]
    }
   ],
   "source": [
    "import paddle2onnx\n",
    "import os\n",
    "model_path_prefix = './model/export/float32'\n",
    "\n",
    "onnx_model = paddle2onnx.export(\n",
    "    model_file=model_path_prefix + \".pdmodel\",\n",
    "    params_file=model_path_prefix + \".pdiparams\",\n",
    "    opset_version=13,\n",
    "    enable_onnx_checker=True,\n",
    ")\n",
    "infer_model_dir = model_path_prefix.rsplit(\"/\", 1)[0]\n",
    "float_onnx_file = os.path.join(infer_model_dir, \"model.onnx\")\n",
    "with open(float_onnx_file, \"wb\") as f:\n",
    "    f.write(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import psutil\n",
    "num_threads = psutil.cpu_count(logical=False)\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.intra_op_num_threads = num_threads\n",
    "predictor = ort.InferenceSession(\n",
    "    onnx_model, sess_options=sess_options, providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2023-05-17 09:51:02,331] [ WARNING]\u001b[0m - Can't find the fast_tokenizer package, please ensure install fast_tokenizer correctly. You can install fast_tokenizer by `pip install fast-tokenizer-python`.\u001b[0m\n",
      "\u001b[32m[2023-05-17 09:51:02,332] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load './model/checkpoint/'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model/checkpoint/', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentence = \"【涉事主体】：工地【主体地址】：荔湾区鹤洞路观鹤小区旁工地【诉求内容】：现希望有关部门尽快跟进处理上述夜间施工问题【事项标签组内容】：发生时间：2020年8月2日22:30开始影响情况：影响周边居民生活【补充信息】：反映2020年8月2日22:30开始，位于荔湾区鹤洞路观鹤小区旁工地。该工地投入施工，产生大量噪音，严重影响附近居民正常生活。【市民回复方式】：电话\"\n",
    "input_data = [sentence]\n",
    "max_seq_length = 300\n",
    "data = tokenizer(\n",
    "    input_data,\n",
    "    max_length=max_seq_length,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_position_ids=False,\n",
    "    return_attention_mask=False,\n",
    ")\n",
    "tokenized_data = {}\n",
    "for tokenizer_key in data:\n",
    "    tokenized_data[tokenizer_key] = np.array(data[tokenizer_key], dtype=\"int64\")\n",
    "preprocess_result = tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 47, 10, 7, 27, 558, 525, 29, 5, 405, 1056, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_result_batch = {}\n",
    "for tokenizer_key in preprocess_result:\n",
    "    preprocess_result_batch[tokenizer_key] = [\n",
    "        preprocess_result[tokenizer_key][0]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [array([    1, 12078,  1157,   104,    57,    82, 12076,    74,    35,\n",
       "            31, 12078,    57,    82,    31,  1392, 12076,    74,  3367,\n",
       "          1108,   121,  2244,  1084,   216,   394,  2244,    96,   121,\n",
       "          1607,    35,    31, 12078,  1005,   323,   103,   390, 12076,\n",
       "            74,    87,   905,   668,     9,   129,    64,   232,   720,\n",
       "           532,  1057,    71,   239,    38,    28,   779,   914,   143,\n",
       "           322,    35,   358,   281, 12078,   104,   236,   275,  1188,\n",
       "           186,   103,   390, 12076,    74,    34,    21,    36,   143,\n",
       "            74,  3615,    17,   585,   136,   249,   139,  1592, 12049,\n",
       "           853,    88,   440,   347,   639,   182,   617,    74,   347,\n",
       "           639,   544,   554,   529,   119,    21,   205, 12078,   807,\n",
       "           684,   212,   399, 12076,    74,   451,  1204,  3615,    17,\n",
       "           585,   136,   249,   139,  1592, 12049,   853,    88,   440,\n",
       "             4,   144,    37,  3367,  1108,   121,  2244,  1084,   216,\n",
       "           394,  2244,    96,   121,  1607,    35,    31, 12043,   310,\n",
       "            35,    31,   447,   109,   322,    35,     4,    66,    21,\n",
       "            19,   150,  2438,   472,     4,   678,   118,   347,   639,\n",
       "           912,   432,   529,   119,   243,   223,    21,   205, 12043,\n",
       "         12078,    99,   119,   381,   441,    58,   220, 12076,    74,\n",
       "           128,   543,     2])],\n",
       " 'token_type_ids': [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_result_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.run(None, preprocess_result_batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.799016,  -6.846868,  -9.159973, ..., -18.293648, -18.144815,\n",
       "       -19.214441], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.799016,  -6.846868,  -9.159973, ..., -18.293648, -18.144815,\n",
       "        -19.214441]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_(x):\n",
    "    \"\"\"\n",
    "    compute sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "threshold = 0.5\n",
    "sigmoid = np.vectorize(sigmoid_)\n",
    "prob = sigmoid(result).reshape([-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99590030e-01, 1.06165077e-03, 1.05154664e-04, ...,\n",
       "       1.13545445e-08, 1.31767034e-08, 4.52141290e-09])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "label_dir = os.path.join('data', \"label.txt\")\n",
    "with open(label_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        label_list.append(line.strip())\n",
    "f.close()\n",
    "label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(prob):\n",
    "    if p > threshold:\n",
    "        label.append(label_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['荔湾区政府', '荔湾区政府##白鹤洞街道', '荔湾区政府##白鹤洞街道##街道综合行政执法办一分队']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
