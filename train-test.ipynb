{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO：尝试拆分句子为短句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from utils import fixText, resetLabelLv2, resetLabelLv3\n",
    "from model import MyFastText\n",
    "\n",
    "TEMP_DIR = './tmp/'\n",
    "\n",
    "def getConfig():\n",
    "    config = {}\n",
    "    with open('./config.json', 'r') as f:\n",
    "        s = f.read()\n",
    "        config = json.loads(s)\n",
    "    return config\n",
    "\n",
    "def split_text(text):\n",
    "    splited_text = re.split(r'\\n+|\\r+|\\r\\n|<br>', text)\n",
    "    while '' in splited_text:\n",
    "        splited_text.remove('')\n",
    "    return splited_text\n",
    "\n",
    "config = getConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.785 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集2拆分后共1285245条文本\n"
     ]
    }
   ],
   "source": [
    "# 读入数据集的json文件，处理成fasttext接口使用的\"文本__label__标签\"形式，以txt文件存储\n",
    "def readDataSet(config):\n",
    "    set1 = []\n",
    "    with open(config['data_path'] + config['dataset_label_1_name'], 'r') as f:\n",
    "        s = f.read()\n",
    "        data_set = json.loads(s)\n",
    "        for id in list(data_set.keys()):\n",
    "            text = data_set[id]['text']\n",
    "            splited_text = split_text(text)\n",
    "            for text in splited_text:\n",
    "                set1.append('__label__' + data_set[id]['label_level_1'] + ' ' + fixText(text))\n",
    "    set2 = []\n",
    "    with open(config['data_path'] + config['dataset_label_2_name'], 'r') as f:\n",
    "        s = f.read()\n",
    "        data_set = json.loads(s)\n",
    "        for id in list(data_set.keys()):\n",
    "            text = data_set[id]['text']\n",
    "            splited_text = split_text(text)\n",
    "            for text in splited_text:\n",
    "                set2.append('__label__' + data_set[id]['label_level_2'] + ' ' + fixText(text))\n",
    "    set3 = []\n",
    "    with open(config['data_path'] + config['dataset_label_3_name'], 'r') as f:\n",
    "        s = f.read()\n",
    "        data_set = json.loads(s)\n",
    "        for id in list(data_set.keys()):\n",
    "            text = data_set[id]['text']\n",
    "            splited_text = split_text(text)\n",
    "            for text in splited_text:\n",
    "                set3.append('__label__' + data_set[id]['label_level_3'] + ' ' + fixText(text))\n",
    "        \n",
    "    try:\n",
    "        os.mkdir(TEMP_DIR)\n",
    "    except:\n",
    "        pass\n",
    "    with open(TEMP_DIR + 'set1.txt', 'w') as f:\n",
    "        print('训练集1拆分后共{}条文本'.format(len(set1)))\n",
    "        for l in set1:\n",
    "            f.write(l + '\\n')\n",
    "    with open(TEMP_DIR + 'set2.txt', 'w') as f:\n",
    "        print('训练集2拆分后共{}条文本'.format(len(set2)))\n",
    "        for l in set2:\n",
    "            f.write(l + '\\n')\n",
    "    with open(TEMP_DIR + 'set3.txt', 'w') as f:\n",
    "        print('训练集3拆分后共{}条文本'.format(len(set2)))\n",
    "        for l in set3:\n",
    "            f.write(l + '\\n')\n",
    "\n",
    "readDataSet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  61624\n",
      "Number of labels: 85\n",
      "Progress: 100.0% words/sec/thread:  193551 lr:  0.000000 avg.loss:  1.508976 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_label_1 = MyFastText()\n",
    "model_label_1.train(\n",
    "    input = TEMP_DIR + 'set1.txt',\n",
    "    lr = config['lr'],\n",
    "    dim = config['hidden_dim'],\n",
    "    epoch = config['epoch'],\n",
    ")\n",
    "model_label_1.save(config['model_path'] + config['model_label_1_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 20M words\n",
      "Number of words:  233142\n",
      "Number of labels: 453\n",
      "Progress:   1.1% words/sec/thread:   37475 lr:  0.098895 avg.loss:  2.244689 ETA:   0h31m31s"
     ]
    }
   ],
   "source": [
    "model_label_2 = MyFastText()\n",
    "model_label_2.train(\n",
    "    input = TEMP_DIR + 'set2.txt',\n",
    "    lr = config['lr'],\n",
    "    dim = config['hidden_dim'],\n",
    "    epoch = config['epoch']\n",
    ")\n",
    "model_label_2.save(config['model_path'] + config['model_label_2_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label_3 = MyFastText()\n",
    "model_label_3.train(\n",
    "    input = TEMP_DIR + 'set3.txt',\n",
    "    lr = config['lr'],\n",
    "    dim = config['hidden_dim'],\n",
    "    epoch = config['epoch']\n",
    ")\n",
    "model_label_3.save(config['model_path'] + config['model_label_2_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './tmp/set1.txt'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.remove(TEMP_DIR + 'set1.txt')\n",
    "    os.remove(TEMP_DIR + 'set2.txt')\n",
    "    os.remove(TEMP_DIR + 'set3.txt')\n",
    "    os.removedirs(TEMP_DIR)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_label1 = MyFastText()\n",
    "model_label1.load_model('./model/model_for_label_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label level 1 accurate: 92.70545264805635% (35534/38330)\n"
     ]
    }
   ],
   "source": [
    "with open(config['data_path'] + 'test_set.json', 'r') as f:\n",
    "    content = json.loads(f.read())\n",
    "\n",
    "total = 0\n",
    "true_label_1 = 0\n",
    "true_label_2 = 0\n",
    "true_label_3 = 0\n",
    "\n",
    "# label_1_dict = {}\n",
    "label_2_dict = {}\n",
    "for id in content.keys():\n",
    "    text = content[id]['text']\n",
    "    \n",
    "    # label_1 = content[id]['label_level_1']\n",
    "    label_2 = resetLabelLv2(content[id]['label_level_2'])\n",
    "    # label_3 = resetLabelLv3(content[id]['label_level_3'])\n",
    "\n",
    "    # if label_1 in label_1_dict:\n",
    "    #     label_1_dict[label_1][0] += 1\n",
    "    # else:\n",
    "    #     label_1_dict[label_1] = [1, 0]\n",
    "\n",
    "    if label_2 in label_2_dict:\n",
    "        label_2_dict[label_2][0] += 1\n",
    "    else:\n",
    "        label_2_dict[label_2] = [1, 0]\n",
    "    \n",
    "    \n",
    "    # predict_1 = model_label1.predict(text)[0][0]\n",
    "    predict_2 = model_label2.predict(text)[0][0]\n",
    "    # predict_3 = model_label3.predict(text)[0][0]\n",
    "\n",
    "    # if(predict_1.replace('__label__', '') == label_1):\n",
    "    #     true_label_1 += 1\n",
    "    #     label_1_dict[label_1][1] += 1\n",
    "    if(predict_2.replace('__label__', '') == label_2):\n",
    "        true_label_2 += 1\n",
    "        label_2_dict[label_2][1] += 1\n",
    "    # if(predict_3.replace('__label__', '') == label_3):\n",
    "    #     true_label_3 += 1\n",
    "    total += 1\n",
    "# print('label level 1 accurate: {}% ({}/{})'.format(true_label_1 * 100 / total, true_label_1, total))\n",
    "print('tag level 2 accurate: {}% ({}/{})'.format(true_label_2 * 100 / total, true_label_2, total))\n",
    "# print('tag level 3 accurate: {}% ({}/{})'.format(true_label_3 * 100 / total, true_lable_3, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "荔湾区政府 35134 / 36018\n",
      "市人力资源社会保障局 1 / 35\n",
      "白云区政府 17 / 213\n",
      "市卫生健康委 77 / 366\n",
      "海珠区政府 5 / 148\n",
      "市总工会 0 / 5\n",
      "市邮政管理局 16 / 35\n",
      "越秀集团 0 / 6\n",
      "广州地铁集团 17 / 66\n",
      "市排水公司 6 / 22\n",
      "广州交投集团 0 / 4\n",
      "珠江实业集团 11 / 25\n",
      "越秀区政府 18 / 303\n",
      "市医保局 2 / 21\n",
      "番禺区政府 5 / 57\n",
      "黄埔区政府 1 / 40\n",
      "市住房城乡建设局 9 / 66\n",
      "市公安局 70 / 150\n",
      "广州供电局有限公司 40 / 67\n",
      "12345热线管理机构 0 / 11\n",
      "市国资委 2 / 5\n",
      "市交通运输局 35 / 95\n",
      "广州市自来水公司 31 / 116\n",
      "广州市消费者委员会 0 / 2\n",
      "穗康码工作专班 0 / 25\n",
      "市规划和自然资源局 1 / 23\n",
      "花都区政府 0 / 29\n",
      "增城区政府 1 / 23\n",
      "市教育局 8 / 25\n",
      "南沙区政府 1 / 34\n",
      "天河区政府 3 / 122\n",
      "广州市公共交通集团有限公司 4 / 20\n",
      "市政务服务数据管理局 0 / 3\n",
      "广州市税务局 11 / 55\n",
      "市民政局 0 / 11\n",
      "市地方金融监管局 3 / 7\n",
      "市商务局 0 / 3\n",
      "市港务局 0 / 1\n",
      "市消防支队 0 / 1\n",
      "市燃气集团 0 / 6\n",
      "市发展改革委 0 / 2\n",
      "市残联 0 / 2\n",
      "市财政局 0 / 1\n",
      "市来穗人员服务管理局 3 / 8\n",
      "市水投集团 0 / 2\n",
      "市建筑集团 0 / 3\n",
      "市城投集团 0 / 1\n",
      "广州轻工工贸集团 1 / 2\n",
      "市文化广电旅游局 0 / 2\n",
      "中国电信广州分公司 0 / 5\n",
      "市农业农村局 0 / 1\n",
      "广州市广播电视台 0 / 1\n",
      "广州环保投资集团 0 / 1\n",
      "广州市烟草专卖局 0 / 3\n",
      "农业银行广东省分行 0 / 2\n",
      "市水务局 0 / 2\n",
      "市林业园林局 1 / 4\n",
      "中国广电广州网络股份有限公司 0 / 2\n",
      "广州海事局 0 / 1\n",
      "市城市管理综合执法局 0 / 6\n",
      "广州交易集团有限公司（广州公共资源交易中心） 0 / 2\n",
      "从化区政府 0 / 4\n",
      "市政府外办 0 / 2\n",
      "市新闻出版局（市版权局） 0 / 1\n",
      "广州海关 0 / 1\n",
      "中国移动广州分公司 0 / 1\n",
      "市市场监管局 0 / 1\n",
      "建设银行广东省分行 0 / 1\n",
      "广州商贸投资控股集团有限公司 0 / 1\n",
      "市气象局 0 / 1\n"
     ]
    }
   ],
   "source": [
    "for label in label_2_dict:\n",
    "    print(label, label_2_dict[label][1], \"/\", label_2_dict[label][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.1 epoch = 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标签数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag 2 num:  459\n",
      "tag 3 num:  970\n"
     ]
    }
   ],
   "source": [
    "print('tag 1 num: ', len(model_label1.labels))\n",
    "# print('tag 2 num: ', len(model_label2.labels))\n",
    "# print('tag 3 num: ', len(model_label3.labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f737a19b7090e33edbc76acd0a86580e3257e24fc38a119320c8b10678394dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
