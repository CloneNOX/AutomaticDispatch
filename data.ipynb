{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入：train_item字典，输出：3个数据集的下标列表\n",
    "def dataEnhance(train_item: dict):\n",
    "    label_count1 = {}\n",
    "    label_count2 = {}\n",
    "    label_count3 = {}\n",
    "\n",
    "    train_set_id_1 = []\n",
    "    train_set_id_2 = []\n",
    "    train_set_id_3 = []\n",
    "\n",
    "    # 调整一级标签===\n",
    "    too_much_tag_id = []\n",
    "    for id in train_item:\n",
    "        if train_item[id]['label_level_1'] == '荔湾区政府':\n",
    "            too_much_tag_id.append(id)\n",
    "        else:\n",
    "            train_set_id_1.append(id)\n",
    "    random.shuffle(too_much_tag_id)\n",
    "    train_set_id_1 += too_much_tag_id[0:30000]\n",
    "\n",
    "    #==============\n",
    "\n",
    "    # 调整二级标签===\n",
    "    train_set_id_2 = train_item.keys()\n",
    "\n",
    "    #==============\n",
    "\n",
    "    # 调整三级标签===\n",
    "    train_set_id_3 = train_item.keys()\n",
    "\n",
    "    #==============\n",
    "    return train_set_id_1, train_set_id_2, train_set_id_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件统计出训练样本191650条\n"
     ]
    }
   ],
   "source": [
    "from data import preProcess\n",
    "total_item = {}\n",
    "for filename in os.listdir(data_path):\n",
    "    if(filename.lower().endswith('.xls') or filename.lower().endswith('xlsx')):\n",
    "        try:\n",
    "            df = pd.read_excel(data_path + filename)\n",
    "            df.columns = [s.strip() for s in df.columns.tolist()]\n",
    "            for row in df.index.values:\n",
    "                id = str(df.loc[row, '工单编号'])\n",
    "                title = df.loc[row, '诉求标题']\n",
    "                if(not pd.isnull(df.loc[row, '市民原始诉求'])):\n",
    "                    text = str(df.loc[row, '市民原始诉求'])\n",
    "                else:\n",
    "                    text = ('' if pd.isnull(df.loc[row, '涉事主体']) else '涉事主体:' + str(df.loc[row, '涉事主体'])) + \\\n",
    "                        ('' if pd.isnull(df.loc[row, '主体地址']) else '主体地址:' + str(df.loc[row, '主体地址'])) + \\\n",
    "                        ('' if pd.isnull(df.loc[row, '事发地点']) else '事发地点:' + str(df.loc[row, '事发地点'])) + \\\n",
    "                        ('' if pd.isnull(df.loc[row, '标签组']) else '标签组:' + str(df.loc[row, '标签组'])) + \\\n",
    "                        ('' if pd.isnull(df.loc[row, '市民诉求']) else '市民诉求:' + str(df.loc[row, '市民诉求'])) + \\\n",
    "                        ('' if pd.isnull(df.loc[row, '补充信息']) else '补充信息:' + str(df.loc[row, '补充信息']))\n",
    "                text = preProcess(text)\n",
    "                label_level_1 = 'undefine' if pd.isnull(df.loc[row, '办理部门一级']) else str(df.loc[row, '办理部门一级'])\n",
    "                label_level_2 = 'undefine' if pd.isnull(df.loc[row, '办理部门二级']) else str(df.loc[row, '办理部门二级'])\n",
    "                label_level_3 = 'undefine' if pd.isnull(df.loc[row, '办理部门三级']) else str(df.loc[row, '办理部门三级'])\n",
    "                total_item[str(id)] = {\n",
    "                    'title': title,\n",
    "                    'text': text,\n",
    "                    'label_level_1': label_level_1,\n",
    "                    'label_level_2': label_level_2,\n",
    "                    'label_level_3': label_level_3\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('文件{}内容有误，请检查'.format(data_path + filename))\n",
    "\n",
    "print('文件统计出训练样本{}条'.format(len(total_item)))\n",
    "\n",
    "if True:\n",
    "    ids = list(total_item.keys())\n",
    "    random.shuffle(ids)\n",
    "    train_item = {}\n",
    "    for id in ids[:len(ids) * 4 // 5]:\n",
    "        train_item[id] = total_item[id]\n",
    "    \n",
    "    test_item = {}\n",
    "    for id in ids[len(ids) * 4 // 5:]:\n",
    "        test_item[id] = total_item[id]\n",
    "\n",
    "    del total_item\n",
    "    \n",
    "train_set_id_1, train_set_id_2, train_set_id_3 = dataEnhance(train_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据增强后1级标签训练集样本39060条\n",
      "数据增强后2级标签训练集样本153320条\n",
      "数据增强后3级标签训练集样本153320条\n"
     ]
    }
   ],
   "source": [
    "train_set_1 = {}\n",
    "train_set_2 = {}\n",
    "train_set_3 = {}\n",
    "for id in train_set_id_1:\n",
    "    train_set_1[id] = copy(train_item[id])\n",
    "    train_set_1[id].pop('label_level_2')\n",
    "    train_set_1[id].pop('label_level_3')\n",
    "for id in train_set_id_2:\n",
    "    train_set_2[id] = copy(train_item[id])\n",
    "    train_set_2[id].pop('label_level_1')\n",
    "    train_set_2[id].pop('label_level_3')\n",
    "for id in train_set_id_3:\n",
    "    train_set_3[id] = copy(train_item[id])\n",
    "    train_set_3[id].pop('label_level_1')\n",
    "    train_set_3[id].pop('label_level_2')\n",
    "print('数据增强后1级标签训练集样本{}条'.format(len(train_set_1)))\n",
    "print('数据增强后2级标签训练集样本{}条'.format(len(train_set_2)))\n",
    "print('数据增强后3级标签训练集样本{}条'.format(len(train_set_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'train_set1.json', 'w') as f:\n",
    "    s = json.dumps(train_set_1, ensure_ascii=False)\n",
    "    f.write(s)\n",
    "with open(data_path + 'train_set2.json', 'w') as f:\n",
    "    s = json.dumps(train_set_2, ensure_ascii=False)\n",
    "    f.write(s)\n",
    "with open(data_path + 'train_set3.json', 'w') as f:\n",
    "    s = json.dumps(train_set_3, ensure_ascii=False)\n",
    "    f.write(s)\n",
    "\n",
    "if True:\n",
    "    with open(data_path + 'test_set.json', 'w') as f:\n",
    "        s = json.dumps(test_item, ensure_ascii=False)\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f737a19b7090e33edbc76acd0a86580e3257e24fc38a119320c8b10678394dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
